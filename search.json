[
  {
    "objectID": "slides/intro.html#who-transport-data-science-team",
    "href": "slides/intro.html#who-transport-data-science-team",
    "title": "Introduction to transport data science",
    "section": "Who: Transport Data Science team",
    "text": "Who: Transport Data Science team\nRobin Lovelace\n\nAssociate Professor of Transport Data Science\nResearching transport futures and active travel planning\nR developer and teacher, author of Geocomputation with R\n\nYuanxuan Yang\n\nLecturer in Data Science of Transport\nNew and Emerging Forms of Data: Investigating novel data sources and their applications in urban mobility and transport planning."
  },
  {
    "objectID": "slides/intro.html#tds-team-ii",
    "href": "slides/intro.html#tds-team-ii",
    "title": "Introduction to transport data science",
    "section": "TDS Team II",
    "text": "TDS Team II\nMalcolm Morgan\n\nSenior researcher at ITS with expertise in routing + web\nDeveloper of the Propensity to Cycle Tool and PBCC\n\nZhao Wang\n\nCivil Engineer and Data Scientist with expertise in machine learning\n\nDemonstrators\n\nJuan Pablo Fonseca Zamora\n\nYou!"
  },
  {
    "objectID": "slides/intro.html#what-is-transport-data-science",
    "href": "slides/intro.html#what-is-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "What is transport data science?",
    "text": "What is transport data science?\n\n\nThe application of data science to transport datasets and problems\nRaising the question‚Ä¶\nWhat is data science?\nA discipline ‚Äúthat allows you to turn raw data into understanding, insight, and knowledge‚Äù (Grolemund, 2016)\n\nIn other words‚Ä¶\n\nStatistics that is actually useful!"
  },
  {
    "objectID": "slides/intro.html#why-take-transport-data-science",
    "href": "slides/intro.html#why-take-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "Why take Transport Data Science",
    "text": "Why take Transport Data Science\n\n\n\n\nNew skills (cutting edge R and/or Python packages)\nPotential for impacts\nAllows you to do new things with data\nIt might get you a job!"
  },
  {
    "objectID": "slides/intro.html#live-demo-npt.scot-web-app",
    "href": "slides/intro.html#live-demo-npt.scot-web-app",
    "title": "Introduction to transport data science",
    "section": "Live demo: npt.scot web app",
    "text": "Live demo: npt.scot web app"
  },
  {
    "objectID": "slides/intro.html#the-history-of-tds",
    "href": "slides/intro.html#the-history-of-tds",
    "title": "Introduction to transport data science",
    "section": "The history of TDS",
    "text": "The history of TDS\n\n2017: Transport Data Science created, led by Dr Charles Fox, Computer Scientist, author of Transport Data Science book (Fox, 2018)\nThe focus was on databases and Bayesian methods\n2019: I inherited the module, which was attended by ITS students\nSummer 2019: Python code published in the module ‚Äòrepo‚Äô:\n\ngithub.com/ITSLeeds"
  },
  {
    "objectID": "slides/intro.html#history-of-tds-ii",
    "href": "slides/intro.html#history-of-tds-ii",
    "title": "Introduction to transport data science",
    "section": "History of TDS II",
    "text": "History of TDS II\n\nJanuary 2020: Available, Data Science MSc course\nMarch 2020: Switch to online teaching\n2021-2023: Updated module, focus on methods\n2024: Switch to combined practical sessions and lectures\n2025+: Expand, online course? book? stay in touch!\n\n\n\nMilestone passed in my academic career, first online-only delivery of lecture @ITSLeeds, seems to have worked, live code demo with #rstats/@rstudio, recording, chat + allüéâThanks students for ‚Äòattending‚Äô + remote participation, we‚Äôll get through this together.#coronavirus pic.twitter.com/wlAUxmZj5r\n\n‚Äî Robin Lovelace (@robinlovelace) March 17, 2020"
  },
  {
    "objectID": "slides/intro.html#essential-reading",
    "href": "slides/intro.html#essential-reading",
    "title": "Introduction to transport data science",
    "section": "Essential reading",
    "text": "Essential reading\n\nChapter 12, Transportation of Geocomputation with R, a open book on geographic data in R (available free online) (Lovelace et al.¬†2019)\nReproducible Road Safety Research with R (RRSRR): https://itsleeds.github.io/rrsrr/"
  },
  {
    "objectID": "slides/intro.html#core-reading-materials",
    "href": "slides/intro.html#core-reading-materials",
    "title": "Introduction to transport data science",
    "section": "Core reading materials",
    "text": "Core reading materials\n\nR for Data Science, an introduction to data science with R (available free online)\nPython equivalent"
  },
  {
    "objectID": "slides/intro.html#optional",
    "href": "slides/intro.html#optional",
    "title": "Introduction to transport data science",
    "section": "Optional",
    "text": "Optional\nThere are many good resources on data science for transport applications. Do your own research and reading! The following are good:\n\nIf you‚Äôre interested in network analysis/Python, see this paper on analysing OSM data in Python (Boeing and Waddell, 2017) (available online)\nIf you‚Äôre interested in the range of transport modelling tools, see Lovelace (2021). \n\nFor more references, see the bibliography at github.com/ITSLeeds/TDS"
  },
  {
    "objectID": "slides/intro.html#objectives",
    "href": "slides/intro.html#objectives",
    "title": "Introduction to transport data science",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the structure of transport datasets\nUnderstand how to obtain, clean and store transport related data\nGain proficiency in command-line tools for handling large transport datasets\nProduce data visualizations, static and interactive\n Learn how to join together the components of transport data science into a cohesive project portfolio"
  },
  {
    "objectID": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "href": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "title": "Introduction to transport data science",
    "section": "Assessment (for those doing this as credit-bearing)",
    "text": "Assessment (for those doing this as credit-bearing)\n\nYou will build-up a portfolio of work\n100% coursework assessed, you will submit by\nWritten in code - will be graded for reproducibility\nCode chunks and figures are encouraged\nYou will submit a non-assessed 2 page pdf + qmd"
  },
  {
    "objectID": "slides/intro.html#schedule",
    "href": "slides/intro.html#schedule",
    "title": "Introduction to transport data science",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "slides/intro.html#feedback",
    "href": "slides/intro.html#feedback",
    "title": "Introduction to transport data science",
    "section": "Feedback",
    "text": "Feedback\n\nThe module is taught by two really well organised and enthusiastic professors, great module, the seminars, structured and unstructured learning was great and well thought out, all came together well\n\n\nI wish this module was 60 credits instead of 15 because i just want more of it."
  },
  {
    "objectID": "s1/index.html",
    "href": "s1/index.html",
    "title": "Seminar 1",
    "section": "",
    "text": "This is a placeholder page; content will be added soon."
  },
  {
    "objectID": "p3/index.html",
    "href": "p3/index.html",
    "title": "Origin-destination data",
    "section": "",
    "text": "You should now be familiar with the basics of R and the tidyverse. If you have not completed these tasks go back and do them first:\n\nRead Chapters 2, 3, and 4 of Reproducible road safety research with R\nRead Chapters 3 and 5 of R for Data Science"
  },
  {
    "objectID": "p3/index.html#pre-requisites",
    "href": "p3/index.html#pre-requisites",
    "title": "Origin-destination data",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nYou need to have a number of packages installed and loaded. Install the packages by typing in the following commands into RStudio (you do not need to add the comments after the # symbol)\nIf you need to install any of these packages use:\n\nRPython\n\n\n\nif (!require(\"pak\")) install.packages(\"pak\")\npak::pkg_install(c(\"sf\", \"tidyverse\", \"remotes\"))\n# GitHub pkgs\npak::pkg_install(\"Nowosad/spDataLarge\")\n\n\nlibrary(sf)          # vector data package \nlibrary(tidyverse)   # tidyverse packages\nlibrary(spData)  # spatial data package \n\n\n\n\n# Install necessary packages (uncomment if not already installed)\n# !pip install geopandas pandas matplotlib seaborn\n\nimport geopandas as gpd       # vector data package\nimport pandas as pd           # data manipulation\nimport matplotlib.pyplot as plt  # plotting\nimport seaborn as sns            # advanced plotting\n# For spatial data, geopandas comes with sample datasets\n# Alternatively, we can use the naturalearth datasets\nimport geopandas.datasets\n\n\n\n\n\nCheck your packages are up-to-date with update.packages() in R (or equivalent in Python)\nCreate a project folder with an appropriate name for this session (e.g.¬†practical2)\nCreate appropriate folders for code, data and anything else (e.g.¬†images)\nCreate a script called learning-OD.R, e.g.¬†with the following command:\n\nmkdir code\ncode code/learning-OD.R # for R\ncode code/learning-OD.py # for Python"
  },
  {
    "objectID": "p3/index.html#basic-sf-operations",
    "href": "p3/index.html#basic-sf-operations",
    "title": "Origin-destination data",
    "section": "2.1 Basic sf operations",
    "text": "2.1 Basic sf operations\nWe will start with a simple map of the world. Load the world object from the spData package. Notice the use of :: to say that you want the world object from the spData package.\n\nRPython\n\n\n\nworld = spData::world\n\n\n\n\nworld = gpd.read_file(\n    'https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip'\n)\n\n\n\n\nUse some basic R functions to explore the world object. e.g.¬†class(world), dim(world), head(world), summary(world). Also view the world object by clicking on it in the Environment panel.\nsf objects can be plotted with plot().\n\nRPython\n\n\n\nplot(world)\n\n\n\n\n\n\n\n\n\n\n\nprint(type(world))       # Equivalent to class(world)\nprint(world.shape)       # Equivalent to dim(world)\nprint(world.head())      # Equivalent to head(world)\nprint(world.describe())  # Equivalent to summary(world)\n\n# Plotting the world GeoDataFrame\nworld.plot(figsize=(12, 8))\nplt.title('World Map')\nplt.show()\n\n\n\n\nNote that this makes a map of each column in the data frame. Try some other plotting options\n\nRPython\n\n\n\nplot(world[3:6])\n\n\n\n\n\n\n\nplot(world[\"pop\"])\n\n\n\n\n\n\n\n\n\n\n\n# Since world is a GeoDataFrame, we can select columns by position\n# However, GeoPandas plots the geometry, so we need to specify columns\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nworld.plot(column='POP_EST', ax=axes[0])\nworld.plot(column='GDP_YEAR', ax=axes[1])\nworld.plot(column='CONTINENT', ax=axes[2])\nplt.show()"
  },
  {
    "objectID": "p3/index.html#basic-spatial-operations",
    "href": "p3/index.html#basic-spatial-operations",
    "title": "Origin-destination data",
    "section": "2.2 Basic spatial operations",
    "text": "2.2 Basic spatial operations\nLoad the nz and nz_height datasets from the spData package.\n\nRPython\n\n\n\nnz = spData::nz\nnz_height = spData::nz_height\n\n\n\n\nnz = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz.gpkg\")\nnz_height = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz_height.gpkg\")\n\n\n\n\nWe can use tidyverse functions like filter and select on sf objects in the same way you did in Practical 1.\n\nRPython\n\n\n\ncanterbury = nz |&gt; filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\n\n\n\n\ncanterbury = nz[nz['Name'] == 'Canterbury']\n\n\n\n\nIn this case we filtered the nz object to only include places called Canterbury and then did and intersection to find objects in the nz_height object that are in Canterbury.\nThis syntax is not very clear. But is the equivalent to\n\nRPython\n\n\n\ncanterbury_height = nz_height[canterbury, , op = st_intersects]\n\n\n\n\ncanterbury_height = gpd.overlay(nz_height, canterbury, how='intersection')\n\n\n\n\nThere are many different types of relationships you can use with op. Try ?st_intersects() to see more. For example this would give all the places not in Canterbury\n\nRPython\n\n\n\nnz_height[canterbury, , op = st_disjoint]\n\n\n\n\ncanterbury_height = gpd.sjoin(nz_height, canterbury, op='intersects')\n\n\n\n\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue. The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string."
  },
  {
    "objectID": "d3/index.html",
    "href": "d3/index.html",
    "title": "Deadline: coursework, 2pm",
    "section": "",
    "text": "This is a placeholder page, contents will be added"
  },
  {
    "objectID": "p1/index.html",
    "href": "p1/index.html",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "",
    "text": "Lecture: an introduction to Transport Data Science (30 min)\n\nSee the slides\n\nQ&A (15 min) \nBreak and networking (15 min) \nData science and a good research question (30 min)\nData science foundations (guided): Project set-up and using RStudio or VS Code as an integrated development environment (30 min)\nFocussed work (1 hr)"
  },
  {
    "objectID": "p1/index.html#agenda",
    "href": "p1/index.html#agenda",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "",
    "text": "Lecture: an introduction to Transport Data Science (30 min)\n\nSee the slides\n\nQ&A (15 min) \nBreak and networking (15 min) \nData science and a good research question (30 min)\nData science foundations (guided): Project set-up and using RStudio or VS Code as an integrated development environment (30 min)\nFocussed work (1 hr)"
  },
  {
    "objectID": "p1/index.html#how-to-come-up-with-a-good-research-question",
    "href": "p1/index.html#how-to-come-up-with-a-good-research-question",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "How to come up with a good research question",
    "text": "How to come up with a good research question\n\nThink about the data you have access to\nThink about the problems you want to solve\nThink about the methods you want to use and skills you want to learn\nThink about how the final report will look and hold-together\n\n\nHow much potential is there for cycling across the transport network?\n\n\n\nHow can travel to schools be made safer?\n\n\nHow can hospitals encourage visitors to get there safely?\n\n\nWhere‚Äôs the best place to build electric car charging points?\nSee openstreetmap.org or search for other open access datasets for more ideas"
  },
  {
    "objectID": "p1/index.html#data-object-manipulation-basics",
    "href": "p1/index.html#data-object-manipulation-basics",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "1.1 Data object manipulation basics",
    "text": "1.1 Data object manipulation basics\n\nUse the $ operator to print the vehicle_type column of crashes.\n\n\nIn R the $ symbol is used to refer to elemements of a list. So the answer is simply:\n\ncrashes$vehicle_type\n\n[1] \"car\"  \"bus\"  \"tank\"\n\n\n\n\nSubset the crashes with the [,] syntax\n\n\nTry out different combinations on the dataframe crashes to see what happens. For example, try:\n\ncrashes[1,]\n\n  casualty_type casualty_age vehicle_type\n1    pedestrian           20          car\n\ncrashes[,1]\n\n[1] \"pedestrian\" \"cyclist\"    \"cat\"       \n\ncrashes[1,1]\n\n[1] \"pedestrian\"\n\n\n\nSubset the object with the [[ syntax.\n\n\nThe [[ operator is used to extract elements from a list. Try:\n\ncrashes[[1]]\n\n[1] \"pedestrian\" \"cyclist\"    \"cat\"       \n\ncrashes[[2]]\n\n[1] 20 40 60\n\n\n\n\nBonus: what is the class() of the objects created by each of the previous exercises?\n\n\nExplore how many R classes you can find\n\n\nBonus (advanced): reproduce the above with Python using the pandas or polars package"
  },
  {
    "objectID": "p1/index.html#data-science-on-real-data",
    "href": "p1/index.html#data-science-on-real-data",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "1.2 Data science on real data",
    "text": "1.2 Data science on real data\nTo get some larger datasets, try the following (from Chapter 8 of RSRR)\n\nRPython\n\n\n\nremotes::install_cran(\"stats19\")\nlibrary(stats19)\nac = get_stats19(year = 2020, type = \"collision\")\nca = get_stats19(year = 2020, type = \"cas\")\nve = get_stats19(year = 2020, type = \"veh\")\n# population hurt by road traffic collisions in 2020:\n(nrow(ca) / 67e6) * 100\n\n[1] 0.1725134\n\n\n\n\nChallenge: reproduce the above code in Python using the pystats19 package\n# Install the package, e.g. with pip\n!pip install pystats19\nimport pystats19\n# See the documentation at https://github.com/Mayazure/py-stats19\n\n\n\nLet‚Äôs go through these exercises together:\n\nSubset the casualty_age object using the inequality (&lt;) so that only elements less than 50 are returned.\nSubset the crashes data frame so that only tanks are returned using the == operator.\nBonus: assign the age of all tanks to 61.\n\n\nTry running the subsetting code on a larger dataset, e.g.¬†the ac object created previously\n\n\nCoerce the vehicle_type column of crashes to the class character.\nCoerce the crashes object into a matrix. What happened to the values?\nBonus: What is the difference between the output of summary() on character and factor variables?\n\n\nWe‚Äôll explore this together"
  },
  {
    "objectID": "p1/index.html#bonus-data-science-and-transport",
    "href": "p1/index.html#bonus-data-science-and-transport",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "2.1 Bonus: data science and transport",
    "text": "2.1 Bonus: data science and transport\n\nWork through Chapter 13 of the book Geocomputation with R, taking care to ask questions about any aspects that you don‚Äôt understand (your homework will be to complete and make notes on the chapter, including reproducible code)."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The timetable below is a user-friently representation of the timetable for the module (see github for .csv and .ics versions). See timetable.leeds.ac.uk for the official timetable. If you spot any discrepancies, please let us know.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "p5/index.html",
    "href": "p5/index.html",
    "title": "Practical 5: Visualising transport data",
    "section": "",
    "text": "This is a placeholder page; content will be added soon."
  },
  {
    "objectID": "marking-criteria.html",
    "href": "marking-criteria.html",
    "title": "Marking Criteria",
    "section": "",
    "text": "Marks are awarded in 4 categories, accounting for the following criteria:\n\n\n\nThe selection and effective use of input datasets that are large (e.g.¬†covering multiple years), complex (e.g.¬†containing multiple variables) and/or diverse (e.g.¬†input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g.¬†by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g.¬†from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n\n\nCreation of figures that are readable and well-described (e.g.¬†with captions and description)\nHigh quality, attractive or advanced techniques (e.g.¬†multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g.¬†mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g.¬†with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "marking-criteria.html#marks",
    "href": "marking-criteria.html#marks",
    "title": "Marking Criteria",
    "section": "",
    "text": "Marks are awarded in 4 categories, accounting for the following criteria:\n\n\n\nThe selection and effective use of input datasets that are large (e.g.¬†covering multiple years), complex (e.g.¬†containing multiple variables) and/or diverse (e.g.¬†input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g.¬†by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g.¬†from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n\n\nCreation of figures that are readable and well-described (e.g.¬†with captions and description)\nHigh quality, attractive or advanced techniques (e.g.¬†multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g.¬†mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g.¬†with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Reading List",
    "section": "",
    "text": "This reading list contains key resources for the Transport Data Science module, organized by topic.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#key-skills",
    "href": "reading.html#key-skills",
    "title": "Reading List",
    "section": "2.1 Key Skills",
    "text": "2.1 Key Skills\n\nQuarto documentation (Allaire et al., 2024)\n\n\nThe software used to create the Transport Data Science course materials and numerous websites, presentations, dashboards, and books, Quarto is a powerful tool for creating reproducible documents with code and data.\nSee the technical writing page of Quarto‚Äôs documentation for key information on how to add references, figure captions, and more.\n\nIntroduction to GitHub (Heis, 2025)\n\nA good starting point for learning how to use GitHub for version control and collaboration. \nSee also their introduction to Devcontainers at docs.github.com/en/codespaces/",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#python",
    "href": "reading.html#python",
    "title": "Reading List",
    "section": "2.2 Python",
    "text": "2.2 Python\n\nCourse Materials for: Geospatial Data Science (Szell, 2025)\n\nCourse materials covering various aspects of geospatial data science, including data analysis, visualization, and working with street networks using Python.\n\nModern Polars (heavey_modern_2025?)\n\nA side-by-side comparison of the Polars and Pandas libraries.\n\nPython Polars: The definitive guide (janssens_python_2025?)\n\nGuide to using the polars for data manipulation in Python, due to be published in February 2025.\n\nA course on Geographic Data Science (Arribas-Bel, 2019)\n\nFree and open source online book on using GeoPandas and other Python libraries for geographic data analysis.\n\nPython for Data Analysis (McKinney, 2022)\n\nDta wrangling with Pandas, NumPy, and Jupyter, written by the creator of the Pandas library.\n\nGeocomputation with Python (Dorman et al., 2025)\n\nResource for working with geographic data using Python, covering both vector and raster data models.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#r",
    "href": "reading.html#r",
    "title": "Reading List",
    "section": "2.3 R",
    "text": "2.3 R\n\nAdvanced R\n\nA comprehensive guide to advanced programming in R, covering topics such as functional programming and object-oriented programming.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "reading.html#miscellaneous",
    "href": "reading.html#miscellaneous",
    "title": "Reading List",
    "section": "5.1 Miscellaneous",
    "text": "5.1 Miscellaneous\n\nData Science for Transport: A Self-Study Guide with Computer Exercises (Fox, 2018)\n\nAn introduction to transport data science with hands-on examples, slightly out of date as of 2025.\n\nReproducible Road Safety Research with R (Lovelace, 2020)\n\nIntroductory guide for analyzing road safety data in R\n\nOpen source tools for geographic analysis in transport planning (Lovelace, 2021)\n\nReview of open source tools available for transport planning and analysis.\n\nPython for Data Science (Turrell et al., 2025)\n\nA modern guide to data science using Python based on R for Data Science, with practical examples and clear explanations.\n\nThe Geography of Transport Systems (Rodrigue et al., 2013)\n\nComprehensive textbook on transport geography and systems\n\nModelling Transport (Ort√∫zar S. and Willumsen, 2001)\n\nFoundational text on transport modeling methods\n\nBuilding Reproducible Analytical Pipelines with R (rodrigues_building?)\n\nA guide to the data engineering side of data science, with a focus on reproducibility and automation.\n\n\nSee the full bibliography on Zotero for more resources, and feel free to suggest additions by opening an issue in the tds issue tracker.",
    "crumbs": [
      "Reading list"
    ]
  },
  {
    "objectID": "d1/index.html",
    "href": "d1/index.html",
    "title": "Welcome and set-up",
    "section": "",
    "text": "Dear Transport Data Science students,\nAs per your timetable, the first session is Thursday 30th January, from 10:00 to 13:00.\nLocation: Richard Hughes Cluster, in the ‚ÄúCloth Workers Link Building‚Äù. If you‚Äôre wondering where that is, you‚Äôre not alone, I‚Äôm not 100% sure. So the first challenge of the module is to ensure that you get there on time, by 09:50, so you have time to get a seat in time for the 10:00 start."
  },
  {
    "objectID": "d1/index.html#homework-for-next-week-deadline-friday-31st-january-1400",
    "href": "d1/index.html#homework-for-next-week-deadline-friday-31st-january-1400",
    "title": "Welcome and set-up",
    "section": "1 Homework for next week (deadline: Friday 31st January, 14:00)",
    "text": "1 Homework for next week (deadline: Friday 31st January, 14:00)\n\nEnsure that you have the timetable stored safely in your calendar, so you do not miss important practicals or seminars.\nEnsure that you have the necessary software installed on your computer and that you have tested that you can use it for the datasets we will be using in the course, see https://itsleeds.github.io/tds/#software-requirements-and-installation for guidance on installing the software you need.\n\nAny issues you have with the software installation, please get in touch with me as soon as possible.\n\n\nTest that you have the necessary software installed by running the following code in R:\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_cran(\"tidyverse\")\nremotes::install_cran(\"osmextract\")\nlibrary(tidyverse)\nlibrary(osmextract)\nlibrary(sf)\n\n\nits = oe_get(\"ITS Leeds\", download_directory = tempdir())\n\n\nfigure = its |&gt;\n  ggplot() +\n  geom_sf(aes(colour = highway)) +\n  theme_void()\n# Save figure\nggsave(\"its.png\", figure, width = 6, height = 4)\n\n\nbrowseURL(\"its.png\")\n\n\nShow the map by executing the following code, which simply prints the map to the screen:\n\nfigure\n\n\n\n\n\n\n\n\nIf you see a map of the area around the Institute for Transport Studies, then you are ready for the first session. If you have any issues, please get in touch with me as soon as possible.\n\nTake a look at the reading list at https://itsleeds.github.io/tds/reading.html and have a read of the Transportation chapter of Geocomputation with R book (you will find the link to the book in the reading list).\nSign-up for a GitHub account if you do not already have one, and ensure that you have access to the TDS GitHub repository where you will find the course materials.\n\nPlease send me an email with you GitHub username so I can add you to the private repository that supports the course."
  },
  {
    "objectID": "p6/index.html",
    "href": "p6/index.html",
    "title": "Practical 6: Project work",
    "section": "",
    "text": "This is a placeholder page; content will be added soon."
  },
  {
    "objectID": "p2/index.html",
    "href": "p2/index.html",
    "title": "Origin-destination data",
    "section": "",
    "text": "You should now be familiar with the basics of R and the tidyverse. If you have not completed these tasks go back and do them first:\n\nRead Chapters 2, 3, and 4 of Reproducible road safety research with R\nRead Chapters 3 and 5 of R for Data Science"
  },
  {
    "objectID": "p2/index.html#pre-requisites",
    "href": "p2/index.html#pre-requisites",
    "title": "Origin-destination data",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nYou need to have a number of packages installed and loaded. Install the packages by typing in the following commands into RStudio (you do not need to add the comments after the # symbol)\nIf you need to install any of these packages use:\n\nRPython\n\n\n\nif (!require(\"pak\")) install.packages(\"pak\")\npak::pkg_install(c(\"sf\", \"tidyverse\", \"remotes\"))\n# GitHub pkgs\npak::pkg_install(\"Nowosad/spDataLarge\")\n\n\nlibrary(sf)          # vector data package \nlibrary(tidyverse)   # tidyverse packages\nlibrary(spData)  # spatial data package \n\n\n\n\n# Install necessary packages (uncomment if not already installed)\n# !pip install geopandas pandas matplotlib seaborn\n\nimport geopandas as gpd       # vector data package\nimport pandas as pd           # data manipulation\nimport matplotlib.pyplot as plt  # plotting\nimport seaborn as sns            # advanced plotting\n# For spatial data, geopandas comes with sample datasets\n# Alternatively, we can use the naturalearth datasets\nimport geopandas.datasets\n\n\n\n\n\nCheck your packages are up-to-date with update.packages() in R (or equivalent in Python)\nCreate a project folder with an appropriate name for this session (e.g.¬†practical2)\nCreate appropriate folders for code, data and anything else (e.g.¬†images)\nCreate a script called learning-OD.R, e.g.¬†with the following command:\n\nmkdir code\ncode code/learning-OD.R # for R\ncode code/learning-OD.py # for Python"
  },
  {
    "objectID": "p2/index.html#basic-sf-operations",
    "href": "p2/index.html#basic-sf-operations",
    "title": "Origin-destination data",
    "section": "2.1 Basic sf operations",
    "text": "2.1 Basic sf operations\nWe will start with a simple map of the world. Load the world object from the spData package. Notice the use of :: to say that you want the world object from the spData package.\n\nRPython\n\n\n\nworld = spData::world\n\n\n\n\nworld = gpd.read_file(\n    'https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip'\n)\n\n\n\n\nUse some basic R functions to explore the world object. e.g.¬†class(world), dim(world), head(world), summary(world). Also view the world object by clicking on it in the Environment panel.\nsf objects can be plotted with plot().\n\nRPython\n\n\n\nplot(world)\n\n\n\n\n\n\n\n\n\n\n\nprint(type(world))       # Equivalent to class(world)\nprint(world.shape)       # Equivalent to dim(world)\nprint(world.head())      # Equivalent to head(world)\nprint(world.describe())  # Equivalent to summary(world)\n\n# Plotting the world GeoDataFrame\nworld.plot(figsize=(12, 8))\nplt.title('World Map')\nplt.show()\n\n\n\n\nNote that this makes a map of each column in the data frame. Try some other plotting options\n\nRPython\n\n\n\nplot(world[3:6])\n\n\n\n\n\n\n\nplot(world[\"pop\"])\n\n\n\n\n\n\n\n\n\n\n\n# Since world is a GeoDataFrame, we can select columns by position\n# However, GeoPandas plots the geometry, so we need to specify columns\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nworld.plot(column='POP_EST', ax=axes[0])\nworld.plot(column='GDP_YEAR', ax=axes[1])\nworld.plot(column='CONTINENT', ax=axes[2])\nplt.show()"
  },
  {
    "objectID": "p2/index.html#basic-spatial-operations",
    "href": "p2/index.html#basic-spatial-operations",
    "title": "Origin-destination data",
    "section": "2.2 Basic spatial operations",
    "text": "2.2 Basic spatial operations\nLoad the nz and nz_height datasets from the spData package.\n\nRPython\n\n\n\nnz = spData::nz\nnz_height = spData::nz_height\n\n\n\n\nnz = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz.gpkg\")\nnz_height = gpd.read_file(\"https://github.com/Nowosad/spData_files/raw/refs/heads/main/data/nz_height.gpkg\")\n\n\n\n\nWe can use tidyverse functions like filter and select on sf objects in the same way you did in Practical 1.\n\nRPython\n\n\n\ncanterbury = nz |&gt; filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\n\n\n\n\ncanterbury = nz[nz['Name'] == 'Canterbury']\n\n\n\n\nIn this case we filtered the nz object to only include places called Canterbury and then did and intersection to find objects in the nz_height object that are in Canterbury.\nThis syntax is not very clear. But is the equivalent to\n\nRPython\n\n\n\ncanterbury_height = nz_height[canterbury, , op = st_intersects]\n\n\n\n\ncanterbury_height = gpd.overlay(nz_height, canterbury, how='intersection')\n\n\n\n\nThere are many different types of relationships you can use with op. Try ?st_intersects() to see more. For example this would give all the places not in Canterbury\n\nRPython\n\n\n\nnz_height[canterbury, , op = st_disjoint]\n\n\n\n\ncanterbury_height = gpd.sjoin(nz_height, canterbury, op='intersects')\n\n\n\n\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue. The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string."
  },
  {
    "objectID": "d2/index.html",
    "href": "d2/index.html",
    "title": "Draft portfolio",
    "section": "",
    "text": "This is a placeholder page, contents will be added"
  },
  {
    "objectID": "s2/index.html",
    "href": "s2/index.html",
    "title": "Seminar 2",
    "section": "",
    "text": "This is a placeholder page; content will be added soon."
  },
  {
    "objectID": "p1/p1project/foundations.html",
    "href": "p1/p1project/foundations.html",
    "title": "1 Python example",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nHello this is some text.\n\ncasualty_type = c(\"cat\", \"dog\", \"person\")\ncasualty_age = seq(from = 20, to = 60, by = 20)\ncrashes = data.frame(casualty_type, casualty_age)\nplot(crashes$casualty_age)\n\n\n\n\n\n\n\n\nSubsetting.\n\ncrashes$casualty_type\n\n[1] \"cat\"    \"dog\"    \"person\"\n\ncrashes[[1]]\n\n[1] \"cat\"    \"dog\"    \"person\"\n\ncrashes[2,1]\n\n[1] \"dog\"\n\n\n\ncrashes |&gt;\n  select(casualty_type)\n\n  casualty_type\n1           cat\n2           dog\n3        person\n\ncrashes |&gt; \n  filter(casualty_age &gt; 35)\n\n  casualty_type casualty_age\n1           dog           40\n2        person           60\n\ncrashes |&gt; \n  filter(casualty_age-20 &gt; 35)\n\n  casualty_type casualty_age\n1        person           60\n\ncrashes |&gt;\n  ggplot() +\n  geom_bar(aes(x = casualty_age, fill = casualty_type))\n\n\n\n\n\n\n\n\n\nac = stats19::get_stats19(year = 2020, type = \"collision\")\n\nFiles identified: dft-road-casualty-statistics-collision-2020.csv\n\n\n   https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-collision-2020.csv\n\n\nData saved at /tmp/RtmpQ1I9G6/dft-road-casualty-statistics-collision-2020.csv\n\n\nReading in: \n\n\n/tmp/RtmpQ1I9G6/dft-road-casualty-statistics-collision-2020.csv\n\n\ndate and time columns present, creating formatted datetime column\n\nac\n\n# A tibble: 91,199 √ó 38\n   accident_index accident_year accident_reference location_easting_osgr\n   &lt;chr&gt;                  &lt;int&gt; &lt;chr&gt;                              &lt;int&gt;\n 1 2020010219808           2020 010219808                         521389\n 2 2020010220496           2020 010220496                         529337\n 3 2020010228005           2020 010228005                         526432\n 4 2020010228006           2020 010228006                         538676\n 5 2020010228011           2020 010228011                         529324\n 6 2020010228012           2020 010228012                         537193\n 7 2020010228014           2020 010228014                         539764\n 8 2020010228017           2020 010228017                         536115\n 9 2020010228018           2020 010228018                         530876\n10 2020010228020           2020 010228020                         529718\n# ‚Ñπ 91,189 more rows\n# ‚Ñπ 34 more variables: location_northing_osgr &lt;int&gt;, longitude &lt;int&gt;,\n#   latitude &lt;int&gt;, police_force &lt;chr&gt;, accident_severity &lt;chr&gt;,\n#   number_of_vehicles &lt;chr&gt;, number_of_casualties &lt;chr&gt;, date &lt;date&gt;,\n#   day_of_week &lt;chr&gt;, time &lt;chr&gt;, local_authority_district &lt;chr&gt;,\n#   local_authority_ons_district &lt;chr&gt;, local_authority_highway &lt;chr&gt;,\n#   first_road_class &lt;chr&gt;, first_road_number &lt;chr&gt;, road_type &lt;chr&gt;, ‚Ä¶\n\ndim(ac)\n\n[1] 91199    38\n\nnames(ac)\n\n [1] \"accident_index\"                             \n [2] \"accident_year\"                              \n [3] \"accident_reference\"                         \n [4] \"location_easting_osgr\"                      \n [5] \"location_northing_osgr\"                     \n [6] \"longitude\"                                  \n [7] \"latitude\"                                   \n [8] \"police_force\"                               \n [9] \"accident_severity\"                          \n[10] \"number_of_vehicles\"                         \n[11] \"number_of_casualties\"                       \n[12] \"date\"                                       \n[13] \"day_of_week\"                                \n[14] \"time\"                                       \n[15] \"local_authority_district\"                   \n[16] \"local_authority_ons_district\"               \n[17] \"local_authority_highway\"                    \n[18] \"first_road_class\"                           \n[19] \"first_road_number\"                          \n[20] \"road_type\"                                  \n[21] \"speed_limit\"                                \n[22] \"junction_detail\"                            \n[23] \"junction_control\"                           \n[24] \"second_road_class\"                          \n[25] \"second_road_number\"                         \n[26] \"pedestrian_crossing_human_control\"          \n[27] \"pedestrian_crossing_physical_facilities\"    \n[28] \"light_conditions\"                           \n[29] \"weather_conditions\"                         \n[30] \"road_surface_conditions\"                    \n[31] \"special_conditions_at_site\"                 \n[32] \"carriageway_hazards\"                        \n[33] \"urban_or_rural_area\"                        \n[34] \"did_police_officer_attend_scene_of_accident\"\n[35] \"trunk_road_flag\"                            \n[36] \"lsoa_of_accident_location\"                  \n[37] \"enhanced_severity_collision\"                \n[38] \"datetime\"                                   \n\n# aggregate this by day to show \n# how crash numbers varied over the year\nac_by_year = ac |&gt;\n  group_by(date) |&gt;\n  summarise(\n    n_crashes = n()\n  )\nac_by_year |&gt;\n  mutate(\n    `N. crashes per year` = n_crashes,\n    `Week average` = zoo::rollmean(n_crashes, 7, na.pad = TRUE),\n    Date = date,\n  ) |&gt; \n  ggplot(aes(x = Date, y = `N. crashes per year`)) +\n  geom_point(alpha = 0.1) +\n  ylim(c(0, NA)) +\n  # geom_smooth() +\n  # weekly rolling average\n  geom_line(aes(Date, `Week average`), colour = \"blue\") +\n  theme_minimal()\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n1 Python example\n\ncasualty_type_py = [\"a\", \"B\", \"c\"]\ncasualty_type_py\n\n['a', 'B', 'c']"
  },
  {
    "objectID": "p1/slides.html#who-transport-data-science-team",
    "href": "p1/slides.html#who-transport-data-science-team",
    "title": "Introduction to transport data science",
    "section": "Who: Transport Data Science team",
    "text": "Who: Transport Data Science team\nRobin Lovelace\n\nAssociate Professor of Transport Data Science\nResearching transport futures and active travel planning\nR developer and teacher, author of Geocomputation with R\n\nYuanxuan Yang\n\nLecturer in Data Science of Transport\nNew and Emerging Forms of Data: Investigating novel data sources and their applications in urban mobility and transport planning."
  },
  {
    "objectID": "p1/slides.html#tds-team-ii",
    "href": "p1/slides.html#tds-team-ii",
    "title": "Introduction to transport data science",
    "section": "TDS Team II",
    "text": "TDS Team II\nMalcolm Morgan\n\nSenior researcher at ITS with expertise in routing + web\nDeveloper of the Propensity to Cycle Tool and PBCC\n\nZhao Wang\n\nCivil Engineer and Data Scientist with expertise in machine learning\n\nDemonstrators\nYou!"
  },
  {
    "objectID": "p1/slides.html#what-is-transport-data-science",
    "href": "p1/slides.html#what-is-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "What is transport data science?",
    "text": "What is transport data science?\n\n\nThe application of data science to transport datasets and problems\nRaising the question‚Ä¶\nWhat is data science?\nA discipline ‚Äúthat allows you to turn raw data into understanding, insight, and knowledge‚Äù (Grolemund, 2016)\n\nIn other words‚Ä¶\n\nStatistics that is actually useful!"
  },
  {
    "objectID": "p1/slides.html#why-take-transport-data-science",
    "href": "p1/slides.html#why-take-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "Why take Transport Data Science",
    "text": "Why take Transport Data Science\n\n\n\n\nNew skills (cutting edge R and/or Python packages)\nPotential for impacts\nAllows you to do new things with data\nIt might get you a job!"
  },
  {
    "objectID": "p1/slides.html#example",
    "href": "p1/slides.html#example",
    "title": "Introduction to transport data science",
    "section": "Example",
    "text": "Example\nData science spin-out company: ImpactML"
  },
  {
    "objectID": "p1/slides.html#data-science-employability",
    "href": "p1/slides.html#data-science-employability",
    "title": "Introduction to transport data science",
    "section": "Data science employability",
    "text": "Data science employability\n\n\n\nThe Bureau of Labor Statistics in the US projects a 35% increase in data science roles in decade 2022-2032.‚Äù Source: visualisecurious.com"
  },
  {
    "objectID": "p1/slides.html#live-demo-npt.scot-web-app",
    "href": "p1/slides.html#live-demo-npt.scot-web-app",
    "title": "Introduction to transport data science",
    "section": "Live demo: npt.scot web app",
    "text": "Live demo: npt.scot web app"
  },
  {
    "objectID": "p1/slides.html#the-history-of-tds",
    "href": "p1/slides.html#the-history-of-tds",
    "title": "Introduction to transport data science",
    "section": "The history of TDS",
    "text": "The history of TDS\n\n2017: Transport Data Science created, led by Dr Charles Fox, Computer Scientist, author of Transport Data Science book (Fox, 2018)\nThe focus was on databases and Bayesian methods\n2019: I inherited the module, which was attended by ITS students\nSummer 2019: Python code published in the module ‚Äòrepo‚Äô:\n\ngithub.com/ITSLeeds"
  },
  {
    "objectID": "p1/slides.html#history-of-tds-ii",
    "href": "p1/slides.html#history-of-tds-ii",
    "title": "Introduction to transport data science",
    "section": "History of TDS II",
    "text": "History of TDS II\n\nJanuary 2020: Available, Data Science MSc course\nMarch 2020: Switch to online teaching\n2021-2023: Updated module, focus on methods\n2024: Switch to combined practical sessions and lectures\n2025+: Expand, online course? book? stay in touch!\n\n\n\nMilestone passed in my academic career, first online-only delivery of lecture (ITSLeeds?), seems to have worked, live code demo with #rstats/(rstudio?), recording, chat + allüéâThanks students for ‚Äòattending‚Äô + remote participation, we‚Äôll get through this together.#coronavirus pic.twitter.com/wlAUxmZj5r\n\n‚Äî Robin Lovelace ((robinlovelace?)) March 17, 2020"
  },
  {
    "objectID": "p1/slides.html#reading-list",
    "href": "p1/slides.html#reading-list",
    "title": "Introduction to transport data science",
    "section": "Reading list",
    "text": "Reading list\nSee the reading list for details"
  },
  {
    "objectID": "p1/slides.html#objectives",
    "href": "p1/slides.html#objectives",
    "title": "Introduction to transport data science",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the structure of transport datasets\nUnderstand how to obtain, clean and store transport related data\nGain proficiency in command-line tools for handling large transport datasets\nProduce data visualizations, static and interactive\n Learn how to join together the components of transport data science into a cohesive project portfolio"
  },
  {
    "objectID": "p1/slides.html#assessment-for-those-doing-this-as-credit-bearing",
    "href": "p1/slides.html#assessment-for-those-doing-this-as-credit-bearing",
    "title": "Introduction to transport data science",
    "section": "Assessment (for those doing this as credit-bearing)",
    "text": "Assessment (for those doing this as credit-bearing)\n\nYou will build-up a portfolio of work\n100% coursework assessed, you will submit by\nWritten in code - will be graded for reproducibility\nCode chunks and figures are encouraged\nYou will submit a non-assessed 2 page pdf + qmd"
  },
  {
    "objectID": "p1/slides.html#feedback",
    "href": "p1/slides.html#feedback",
    "title": "Introduction to transport data science",
    "section": "Feedback",
    "text": "Feedback\n\nThe module is taught by two really well organised and enthusiastic professors, great module, the seminars, structured and unstructured learning was great and well thought out, all came together well\n\n\nI wish this module was 60 credits instead of 15 because i just want more of it."
  },
  {
    "objectID": "p1/slides.html#timetable",
    "href": "p1/slides.html#timetable",
    "title": "Introduction to transport data science",
    "section": "Timetable",
    "text": "Timetable\nSee the schedule for details"
  },
  {
    "objectID": "p1/slides.html#what-is-science",
    "href": "p1/slides.html#what-is-science",
    "title": "Introduction to transport data science",
    "section": "What is science?",
    "text": "What is science?\n\n\n\nScientific knowledge is hypotheses that can be falsified\nScience is the process of generating falsifiable hypotheses and testing them\nIn a reproducible way\nSystematically\n\n\n\n\nFalsifiability is central to the scientific process (Popper 1959)\nAll of which requires software conducive to reproducibility"
  },
  {
    "objectID": "p1/slides.html#transport-planning-software",
    "href": "p1/slides.html#transport-planning-software",
    "title": "Introduction to transport data science",
    "section": "Transport planning software",
    "text": "Transport planning software\nTransport modelling software products are a vital component of modern transport planning and research.\n\nThey generate the evidence base on which strategic investments are made and, furthermore,\nprovide a powerful mechanism for researching alternative futures.\n\nIt would not be an overstatement to say that software determines the range of futures that are visible to policymakers. This makes status of transport modelling software and how it may evolve in the future important questions.\nWhat will transport software look like? What will their capabilities be? And who will control? Answers to each of these questions will affect the future of transport systems.\n\nPremise: transport planning/modelling software used in practice will become is becoming increasingly data-driven, modular and open."
  },
  {
    "objectID": "p1/slides.html#current-transport-software",
    "href": "p1/slides.html#current-transport-software",
    "title": "Introduction to transport data science",
    "section": "Current transport software",
    "text": "Current transport software\n\n\n\n\n\n\n\n\n\n4-stage model still dominates transport planning models (Boyce and Williams 2015)"
  },
  {
    "objectID": "p1/slides.html#the-four-stage-model",
    "href": "p1/slides.html#the-four-stage-model",
    "title": "Introduction to transport data science",
    "section": "The four stage model",
    "text": "The four stage model\n\nImpacts the current software landscape\nDominated by a few proprietary products\nLimited support community online\nHigh degree of lock-in\nLimited cross-department collaboration"
  },
  {
    "objectID": "p1/slides.html#existing-products",
    "href": "p1/slides.html#existing-products",
    "title": "Introduction to transport data science",
    "section": "Existing products",
    "text": "Existing products\nSample of transport modelling software in use by practitioners. \n\n\n\n\n\nSoftware\nCompany/Developer\nCompany HQ\nLicence\nCitations\n\n\n\n\nVisum\nPTV\nGermany\nProprietary\n1810\n\n\nMATSim\nTU Berlin\nGermany\nOpen source (GPL)\n1470\n\n\nTransCAD\nCaliper\nUSA\nProprietary\n1360\n\n\nSUMO\nDLR\nGermany\nOpen source (EPL)\n1310\n\n\nEmme\nINRO\nCanada\nProprietary\n780\n\n\nCube\nCitilabs\nUSA\nProprietary\n400\n\n\nsDNA\nCardiff University\nUK\nOpen source (GPL)\n170"
  },
  {
    "objectID": "p1/slides.html#user-support",
    "href": "p1/slides.html#user-support",
    "title": "Introduction to transport data science",
    "section": "User support",
    "text": "User support\nGetting help is vital for leaning/improving software\n\n‚Äú10-Hour Service Pack $2,000‚Äù (source: caliper.com/tcprice.htm)"
  },
  {
    "objectID": "p1/slides.html#online-communities",
    "href": "p1/slides.html#online-communities",
    "title": "Introduction to transport data science",
    "section": "Online communities",
    "text": "Online communities\n\ngis.stackexchange.com has 21,314 questions\nr-sig-geo has 1000s of posts\nRStudio‚Äôs Discourse community has 65,000+ posts already!\nNo clear transport equivalent (e.g.¬†earthscience.stackexchange.com is in beta)\nSolution: build our own community!\n\nSee https://github.com/ITSLeeds/TDS/issues for example\nPlace for discussions: https://github.com/itsleeds/tds/discussions"
  },
  {
    "objectID": "p1/slides.html#best-way-to-get-support-is-peer-to-peer",
    "href": "p1/slides.html#best-way-to-get-support-is-peer-to-peer",
    "title": "Introduction to transport data science",
    "section": "Best way to get support is peer-to-peer:",
    "text": "Best way to get support is peer-to-peer:\n\nSource: https://community.rstudio.com/about"
  },
  {
    "objectID": "p1/slides.html#how-is-data-science-used-in-the-pct",
    "href": "p1/slides.html#how-is-data-science-used-in-the-pct",
    "title": "Introduction to transport data science",
    "section": "How is data science used in the PCT?",
    "text": "How is data science used in the PCT?\n\nIt‚Äôs all reproducible, e.g.:\nFind commuting desire lines in West Yorkshire between 1 and 3 km long in which more people drive than cycle:"
  },
  {
    "objectID": "p1/slides.html#visualising-data",
    "href": "p1/slides.html#visualising-data",
    "title": "Introduction to transport data science",
    "section": "Visualising data",
    "text": "Visualising data\nA fundamental part of data science is being able to understand your data.\nThat requires visualisation, R is great for that:"
  },
  {
    "objectID": "p1/slides.html#interactively",
    "href": "p1/slides.html#interactively",
    "title": "Introduction to transport data science",
    "section": "Interactively",
    "text": "Interactively"
  },
  {
    "objectID": "p1/slides.html#processing-data-with-code",
    "href": "p1/slides.html#processing-data-with-code",
    "title": "Introduction to transport data science",
    "section": "Processing data with code",
    "text": "Processing data with code\n\nNow we have data in our computer, and verified it works, we can use it\nWhich places are most car dependent?"
  },
  {
    "objectID": "p1/slides.html#checking-the-results",
    "href": "p1/slides.html#checking-the-results",
    "title": "Introduction to transport data science",
    "section": "Checking the results:",
    "text": "Checking the results:"
  },
  {
    "objectID": "p1/slides.html#r-vs-python",
    "href": "p1/slides.html#r-vs-python",
    "title": "Introduction to transport data science",
    "section": "R vs Python",
    "text": "R vs Python\n\nLots of debate on this topic - see https://blog.usejournal.com/python-vs-and-r-for-data-science-833b48ccc91d\n\nHow to decide?\n\nIf priority: getting things done quick (with support from me ;) go with R\nIf you already know Python and are 100% confident you can generate reproducible results, go with that\nIf you want to be avant-garde and try something else like Julia, do it (as long as it‚Äôs reproducible)\n\n\nGamification\n\n\n\n\n\n\n\n\n\n\nCompletely open source, written in rust\nSource: video at https://github.com/dabreegster/abstreet/#ab-street"
  },
  {
    "objectID": "p1/slides.html#summary",
    "href": "p1/slides.html#summary",
    "title": "Introduction to transport data science",
    "section": "Summary",
    "text": "Summary\n\nWalk and understand the data before doing complex things\nVisualise the data, ask questions of it, descriptive stats\nOnly then add complexity to your analysis\nStarting point for this: Transport chapter of Geocomputation with R (lovelace_geocomputation_2018?)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "A module on using data science to solve transport problems. This couse is based at the University of Leeds‚Äô Institute for Transport Studies. It has evolved over a decade of teaching and research in the field and aims to teach you up-to-date and future-proof skills with practical examples and reproducible workflows using industry-standard data science tools.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#hardware",
    "href": "index.html#hardware",
    "title": "Transport Data Science",
    "section": "Hardware",
    "text": "Hardware\nAccess to a computer that you have permission to install software on, with at least 8 GB of RAM, is highly recommended. You could use a cloud-based service such as RStudio Cloud, Google Colab, or GitHub Codespaces, but you would need to be comfortable with using these services and would miss out on some of the benefits of using your own computer.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#command-line-experience",
    "href": "index.html#command-line-experience",
    "title": "Transport Data Science",
    "section": "Command-line experience",
    "text": "Command-line experience\nYou should be comfortable with computing in general, for example creating folders, moving files, and installing software. You should be comfortable with using command line interfaces such as PowerShell in Windows, Terminal in macOS, or the Linux shell.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#data-science-experience-prerequisites",
    "href": "index.html#data-science-experience-prerequisites",
    "title": "Transport Data Science",
    "section": "Data science experience prerequisites",
    "text": "Data science experience prerequisites\nPrior experience of using R or Python (e.g.¬†having used it for work, in previous degrees or having completed an online course) is essential.\nStudents can demonstrate this by showing evidence that they have worked with R before, have completed an online course such as the first 4 sessions in the RStudio Primers series or DataCamp‚Äôs Free Introduction to R course.\nEvidence of substantial programming and data science experience in previous professional or academic work, in languages such as R or Python, also constitutes sufficient pre-requisite knowledge for the course.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#quickstart-with-github-codespaces",
    "href": "index.html#quickstart-with-github-codespaces",
    "title": "Transport Data Science",
    "section": "Quickstart with GitHub Codespaces",
    "text": "Quickstart with GitHub Codespaces\nYou can use GitHub Codespaces to get started with the course materials in a cloud-based environment. Sign-up to GitHub, fork the repository, and click the ‚ÄúOpen in GitHub Codespaces‚Äù button above to get started. You can also use the following link:\n\n\n\nOpen in GitHub Codespaces",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#r",
    "href": "index.html#r",
    "title": "Transport Data Science",
    "section": "R",
    "text": "R\nInstall a recent version of R (4.3.0 or above) and RStudio (recommended) or another IDE such as VS Code (if you have prior experience with it) with the the following links:\n\nR from cran.r-project.org\nRStudio from rstudio.com (recommended) or VS Code with the R extension installed.\nR packages, which can be installed by opening RStudio and typing install.packages(\"stats19\") in the R console, for example.\nTo install all the dependencies for the module, run the following command in the R console:\n\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"itsleeds/tds\")\n\nSee Section 1.5 of the online guide Reproducible Road Safety Research with R for instructions on how to install key packages we will use in the module.1",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Transport Data Science",
    "section": "Python",
    "text": "Python\nIf you choose to use Python, you should be able to install it and manage your own Python environment, including installing packages and dealing with package conflicts. If you use Python we recommend using an environment manager such as pixi (which can manage both R and Python environments) or Docker (best practice for reproducibility and isolation).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#docker-advanced",
    "href": "index.html#docker-advanced",
    "title": "Transport Data Science",
    "section": "Docker (advanced)",
    "text": "Docker (advanced)\nWe maintain a Docker image that contains all the software you need to complete the course with VS Code, quarto and a Devcontainer set-up. Advantages of this approach include that it ensures reproducibility and can save time installing software. Disadvantages include that it can be hard to install Docker and can be difficult to use if you are not familiar with Docker. We therefore recommend this approach only for people who are confident with Docker and willing to invest time in learning how to use it. See the Docker installation instructions, the devcontainers documentation on github.com and the tds Dockerfile and devcontainer.json for guidance on getting started with Docker.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Transport Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n For further guidance on setting-up your computer to run R and RStudio for spatial data, see these links, we recommend Chapter 2 of Geocomputation with R (the Prerequisites section contains links for installing spatial software on Mac, Linux and Windows): https://geocompr.robinlovelace.net/spatial-class.html and Chapter 2 of the online book Efficient R Programming, particularly sections 2.3 and 2.5, for details on R installation and set-up and the project management section.‚Ü©Ô∏é",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "p4/index.html",
    "href": "p4/index.html",
    "title": "Practical 4: Routing",
    "section": "",
    "text": "This is a placeholder page; content will be added soon."
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Introduction to Transport Data Science"
  }
]